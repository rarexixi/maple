# 总体架构

启动引擎的角色可以分为 `group` 和 `user`

启动的引擎可以分为一次性的和常驻的

一次性的主要是为了跑任务，常驻的主要是为了交互式开发

user 必须属于某个 group

group 上配置空闲引擎的个数/比例，以及 group 下用户是否可以启用自己的引擎
每个引擎可以接收的任务上限

# 用户请求任务

请求需要带上 group、user、对接应用、作业类型、请求唯一ID、请求描述，请求类型，执行类型，webhook(回调地址)

1. 将写入数据库，返回任务 ID
2. 将任务 ID 放到 redis 队列，返回给用户任务 ID

## 常驻任务

![](./assets/img/img1.svg)

调度器持续消费 redis 队列

队列标识：cluster + queue + 引擎种类 + 引擎版本 + 任务类型(once,resident) + 来源应用 + group + 优先级
例：hadoop-root.default-spark-3.2.3-once-schedule-maple-1

**消费任务**

1. 加锁
2. 获取空闲引擎
   1. 如果不存在空闲引擎，判断队列是否有资源，有资源则启动新引擎，否则直接返回
   2. 如果存在空闲引擎，直接提交任务
3. 释放锁

**引擎执行**

执行完成后，通过 webhook 通知到调用方

1. 加锁
2. 将引擎任务数 - 1
3. 释放锁

## 单次任务

需要给定资源配置

调度器持续消费 redis 队列，

**消费任务**

1. 加锁
2. 判断是否有资源
   1. 将任务提交到新引擎
3. 释放锁

**引擎执行**

执行完成后，通过 webhook 通知到调用方
